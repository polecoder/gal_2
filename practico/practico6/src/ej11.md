# Ejercicio 11

## Consigna

Sea $V$ un espacio vectorial de dimensión finita con producto interno, $S \subset V$ un subespacio vectorial y $P_S(v)$ la proyección ortogonal de $v$ sobre $S$; es decir, $P_S(v)$ es el único vector que verifica que $P_S(v) \in S$ y $v - P_S(v) \in S^\perp$.

Probar que:

1. $P_S(s) = s \quad\forall s \in S$  
2. $P_S(v) = \vec{0} \quad\forall v \in S^\perp$
3. $P_S : V \to V$ es transformación lineal
4. Hallar la matriz asociada de $P_S$ en una base unida de $S$ con $S^\perp$  
5. Hallar el núcleo e imagen de $P_S$  
6. Hallar valores y subespacios propios de $P_S$. Es $P_S$ diagonalizable?
7. $\|v\|^2 = \|P_S(v)\|^2 + \|P_{S^\perp}(v)\|^2 \quad\forall v \in V$
8. $\|P_S(v)\| \le \|v\|\quad\forall v\in V$
9. $\langle v,\ P_S(v) \rangle = \|P_S(v)\|^2 \quad\forall v \in V$

## Resolución

Dada una base ortonormal $\mathcal{B}=\{s_1,\ldots,s_r\}$ de $S$, definimos $P_S(v)$ con $v\in V$ como:

$P_S(v)=\sum_{i=1}^{r}\left<v, s_i\right>s_i$

### Parte 1

Queremos probar que $P_S(s) = s \quad\forall s \in S$.

Por definición de proyección, $P_S(s)$ es el único vector en $S$ que cumple que: $s-P_S(s)\in S^{\perp}$.
Ahora, considerando que $s-s=\vec{0}\in S^{\perp}$, podemos concluir que necesariamente $P_S(s)=s$, para todo $s\in S$ pues consideramos un $s$ cualquiera.

### Parte 2

Queremos probar que $P_S(v) = \vec{0} \quad\forall v \in S^\perp$.

Por definición de proyección, $P_S(v)$ es el único vector en $S$ que cumple que: $v-P_S(v)\in S^{\perp}$.
Ahora, considerando que $v-\vec{0}=v\in S^{\perp}$ (por hipótesis), podemos concluir que $P_S(v)=\vec{0}$, para todo $s\in S$ pues consideramos un $s$ cualquiera.

### Parte 3

Queremos probar que $P_S : V \to V$ es transformación lineal

Entonces, dados $v,w\in V$ y $\alpha\in\mathbb{K}$ se tienen que cumplir las siguientes afirmaciones:

1. $P_S(v+w)=P_S(v)+P_S(w)$
2. $P_S(\alpha v)= \alpha P_S(v)$

Consideremos la definición de $P_S(v)$ para una base ortonormal $\mathcal{B}=\{s_1,\ldots,s_r\}$ de $S$, dada por

$P_S(v)=\sum_{i=1}^{r}\left<v, s_i\right>s_i$

#### Subparte 1

Desarrollemos:

$$
\begin{aligned}
&P_S(v+w)\\
&=\scriptstyle{(\text{definición de proyección ortogonal})}\\
&\sum_{i=1}^{r}\left<v+w, s_i\right>s_i\\
&=\scriptstyle{(\text{propiedades de producto interno})}\\
&\sum_{i=1}^{r}(\left<v, s_i\right> + \left<w, s_i\right>)s_i\\
&=\scriptstyle{(\text{distribuyendo})}\\
&\sum_{i=1}^{r}\left<v, s_i\right>s_i + \left<w, s_i\right>s_i\\
&=\scriptstyle{(\text{separando la sumatoria})}\\
&\sum_{i=1}^{r}\left<v, s_i\right>s_i + \sum_{i=1}^{r} \left<w, s_i\right>s_i\\
&=\scriptstyle{(\text{definición de proyección ortogonal})}\\
&P_S(v)+P_S(w)
\end{aligned}
$$

Lo que concluye esta subparte.

#### Subparte 2

Desarrollemos:

$$
\begin{aligned}
&P_S(\alpha v)\\
&=\scriptstyle{(\text{definición de proyección ortogonal})}\\
&\sum_{i=1}^{r}\left<\alpha v, s_i\right>s_i\\
&=\scriptstyle{(\text{propiedades del producto interno})}\\
&\sum_{i=1}^{r}\alpha\left<v, s_i\right>s_i\\
&=\scriptstyle{(\text{propiedades la sumatoria})}\\
&\alpha\sum_{i=1}^{r}\left<v, s_i\right>s_i\\
&=\scriptstyle{(\text{definición de proyección ortogonal})}\\
&\alpha P_S(v)
\end{aligned}
$$

### Parte 4

Queremos hallar la matriz asociada de $P_S$ en una base construida uniendo una base de $S$ con una de $S^{\perp}$.

Consideremos bases de $S$ y $S^{\perp}$:

- $\mathcal{B}^S=\{v_1,\ldots,v_r\}$
- $\mathcal{B}^{S^{\perp}}=\{v_{r+1},\ldots,v_n\}$

Sea $\mathcal{B} = \{v_1,\ldots,v_r,v_{r+1},\ldots,v_n\}$ base de $V$.
Calculemos las columnas de la matriz asociada:
- $coord_{\mathcal{B}}(T(v_1))= coord_{\mathcal{B}}(v_1)=(1,\ldots,0)$
- $\cdots$
- $coord_{\mathcal{B}}(T(v_r))= coord_{\mathcal{B}}(v_r)=(0,\ldots,1,\ldots,0)$
- $coord_{\mathcal{B}}(T(v_{r+1}))= coord_{\mathcal{B}}(v_{r+1})=(0,\ldots,0)$
- $\cdots$
- $coord_{\mathcal{B}}(T(v_n))= coord_{\mathcal{B}}(v_n)=(0,\ldots,0)$

Esto es así por las propiedades 1 y 2, considerando que:

1. $v_1,\ldots,v_r\in S$
1. $v_{r+1},\ldots,v_n\in S^{\perp}$

Entonces la matriz asociada ${}_{\mathcal{B}}(P_S)_{\mathcal{B}}$ se ve de la siguiente forma:

$$
{}_{\mathcal{B}}(P_S)_{\mathcal{B}}=\begin{pmatrix}I_{r\times r}&0_{n-r\times n-r}\\0_{n-r\times n-r}&0_{n-r\times n-r}\end{pmatrix}_{n\times n}
$$

### Parte 5

Queremos hallar el núcleo e imagen de $P_S$.
Describamos los conjuntos para ver que podemos decir de ellos:

- $Ker(P_S)=\{P_S(v)=0\mid v\in V\}$
- $Im(P_S)=\{P_S(v)\mid v\in V\}$

Ahora, para el nucleo, por la propiedad 2, podemos concluir que ese conjunto es exactamente $S^{\perp}$.

En segundo lugar considerando la imagen, como $P_S$ proyecta en $S$, $Im(P_S)=S$.

### Parte 6

Queremos hallar valores y subespacios propios para $P_S$. Utilizaremos el razonamiento hecho en la parte 4.

#### Valores propios

Observamos que la matriz asociada que hallamos en la parte 4 es diagonal. Vemos que los valores propios son:

- $\lambda_1=1$, con $mg(\lambda)=r$
- $\lambda_2=0$, con $mg(\lambda)=n-r$

#### Subespacios asociados

- El subespacio asociado a $\lambda_1=1$ es $S$, pues está asociado a una base de vectores propios que corresponde exactamente a la base de $S$.
- El subespacio asociado a $\lambda_2=0$ es $S^{\perp}$, pues está asociado a una base de vectores propios que corresponde exactamente a la base de $S^{\perp}$.

#### Diagonalización

Claramente es diagonalizable pues hallamos una base para la cual la matriz asociada de $P_S$ es diagonal.

### Parte 7

Queremos probar que  $\|v\|^2 = \|P_S(v)\|^2 + \|P_{S^\perp}(v)\|^2 \quad\forall v \in V$.

Veamos que podemos escribir todo $v\in V$ como $v=P_S(v)+P_{S^{\perp}}(v)$, entonces:

$$
\begin{aligned}
&\|v\|^2\\
&=\scriptstyle{(\text{norma inducida por el producto interno})}\\
&\left<v, v\right>\\
&=\scriptstyle{(\text{descomposición de }v)}\\
&\left<P_S(v)+P_{S^{\perp}}(v), P_S(v)+P_{S^{\perp}}(v)\right>\\
&=\scriptstyle{(\text{propiedades del producto interno})}\\
&\left<P_S(v), P_S(v)\right>+\left<P_S(v), P_{S^{\perp}}(v)\right>+\left<P_{S^{\perp}}(v), P_S(v)\right>+\left<P_{S^{\perp}}(v), P_{S^{\perp}}(v)\right>\\
&=\scriptstyle{(P_S(v)\perp P_{S^{\perp}}(v))}\\
&\left<P_S(v), P_S(v)\right>+\left<P_{S^{\perp}}(v), P_{S^{\perp}}(v)\right>\\
&=\scriptstyle{(\text{norma inducida por el producto interno})}\\
&\|P_S(v)\|^2+\|P_{S^{\perp}}(v)\|^2
\end{aligned}
$$

### Parte 8

Queremos probar que $\|P_S(v)\| \le \|v\|\quad\forall v\in V$.

Veamos que podemos escribir todo $v\in V$ como $v=P_S(v)+P_{S^{\perp}}(v)$, entonces:

$$
\begin{aligned}
&\|v\|^2\\
&=\scriptstyle{(\text{norma inducida por el producto interno})}\\
&\left<v, v\right>\\
&=\scriptstyle{(\text{descomposición de }v)}\\
&\left<P_S(v)+P_{S^{\perp}}(v), P_S(v)+P_{S^{\perp}}(v)\right>\\
&=\scriptstyle{(\text{propiedades del producto interno})}\\
&\left<P_S(v), P_S(v)\right>+\left<P_S(v), P_{S^{\perp}}(v)\right>+\left<P_{S^{\perp}}(v), P_S(v)\right>+\left<P_{S^{\perp}}(v), P_{S^{\perp}}(v)\right>\\
&=\scriptstyle{(P_S(v)\perp P_{S^{\perp}}(v))}\\
&\left<P_S(v), P_S(v)\right>+\left<P_{S^{\perp}}(v), P_{S^{\perp}}(v)\right>\\
&=\scriptstyle{(\text{norma inducida por el producto interno})}\\
&\|P_S(v)\|^2+\|P_{S^{\perp}}(v)\|^2\\
&\geq\scriptstyle{(\text{considerando que }P_{S^{\perp}}\geq 0)}\\
&\|P_S(v)\|^2
\end{aligned}
$$

Entonces concluimos que $\|P_S(v)\| \le \|v\|\quad\forall v\in V$.

### Parte 9

Queremos probar que $\langle v,\ P_S(v) \rangle = \|P_S(v)\|^2 \quad\forall v \in V$.

Consideremos $v= P_S+(v-P_S)$, con $v-P_S\in S^{\perp}$:

$$
\begin{aligned}
&\left<v, P_S(v)\right>\\
&=\scriptstyle{(\text{desarrollo de }v)}\\
&\left<P_S(v), P_S(v)\right> + \left<v-P_S(v), P_S(v)\right>\\
&=\scriptstyle{(v-P_S(v)\in S^{\perp}; P_S(v)\in S)}\\
&\left<P_S(v), P_S(v)\right>+0\\
&=\scriptstyle{(\text{norma inducida por el producto interno})}\\
&\|P_S(v)\|^2
\end{aligned}
$$